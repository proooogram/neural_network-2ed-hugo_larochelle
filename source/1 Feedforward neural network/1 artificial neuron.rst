.. 注释 ():同位语  []:省略了 但其实是不可少的  {}:译者的话  "(x1,x2)":平面上的点坐标为(x1,x2)

人工神经元
==========================================================

.. toctree::

| 在这个视频，我们将介绍人工神经元的概念，它是基本建筑块，我们将用(基本建筑块)构建复杂神经网络

.. image:: picture/1_01_artificial_neuron.pdf.page2.png 
   :scale: 50%

| 一个人工神经元是简单的一个计算单元,它(计算单元)将做一个特殊计算,(特殊计算)基于其他单元,它(计算单元)连到[其他单元]
| 上图是单人工神经元案例
| 它(神经元)直连在一个某个对象的输入描述，我们想抽取信息从[某个对象]
| 这个输入描述，我们叫它x
| 这儿两个x本质上是一个向量, (向量)包含标量x1到xd, d是此向量大小
| 注意，我将用粗体来写向量和矩阵 并且 每当无(粗体)时对应到一个标量值
| 所以xi是向量x的第i个元素
| 这个神经元将读信息从x向量并执行一个特殊计算，它(特殊计算)将决定神经元的值 
| 并且这个值将完全表明，是否某个特殊特征或信息出现[与否]在此对象，关于此主题(的特征或信息)，我们正在操纵的(主题)，被向量x描述的(对象)
| 现在 将执行的计算 能被拆成两步
| 此神经元的预激活计算会是第一个
| 有时 在你看讲义时 你看到输入激活的表达(相对预激活) 
| 但是 我将避免用输入激活 因为对于x我们经常也用词输入去描述
| 所以 我讨论这为预激活
| 并且 预激活，我记(预激活)作a(x)，对于一个给定的神经元
| ( a(x) )简单地会是标量b + 权重向量w乘输入向量x, b是神经元偏执，
| 所以 我们可以写它(式子b+ sigma w*x)成向量形式
| 或者 我们能写它(式子)成多标量形式,我们有 b + 对于所有向量内下标i w的第i个元素乘以x的第i个元素
| 然后 从这个(预激活a(x)=b+w*x) 神经元将计算它的激活，(激活)有时是作为神经元输出激活 按照 简单取预激活传递给激活函数g 
| g函数就是激活函数 并且 我们将可能用不同的激活函数

| 输出激活我经常所知它是h(x) . 如果我不说输出那就只剩激活了 . 如果我只说激活, 我的意思是一个给定神经元的输出激活
| {完成h(x)=g(a(x))=g(b+wx)的解释:} 所以 如果我只放a(x)在g中 很明确地 激活函数g将应用在输入向量的线性变换上，(向量)由元素xi组成

| {解释权重w} w我们将其作为连接权重的向量 
| 这是因为 在这里的可视化{上图} 我们可以把w的元素当作神经元间连接的强度 ,其他神经元(图中有曲线的神经元)到本神经元(x1)的连接 
| 这个案例中 这些(图中xi)神经元是指输入神经元 这里(图中xi)是神经元,(神经元)取输入向量中每一个元素的值 
| 所以w是连接权重的向量 
| {解释偏执b} b是偏执 因为如果我们没有输入{看公式a(x)=b+sigma w*x} b将是预激活
| 观察一个特定输入{w非0} 我们将完全远离初始值b, 神经元预激活的(初始值b)  然后像我说过的 g是激活函数


.. image:: picture/1_01_artificial_neuron.pdf.page3.png 
   :scale: 50%
 
| 这是一个神经元激活的二维可视化 
| 想象我们有一个向量, (该向量)由两个元素x1和x2组成
| 我们注意y 这我应写h(x)  (而不是y) 作为神经元的输出激活
| 这{y轴}我们注意到的第一个事情 是 在这个轴这 我们获得值,介于-1到+1之间(的值) 
| 这是因为 在这个特定例子中 我们能获得的值{激活}的范围  一旦我们传递预激活通过激活函数{从而能获得的激活值} 是一个数 (数)在-1到+1之间
| 现在我们看到 对于x的不同值 我们得到一个不同输出 
| 并且 特别地 对于所有值，躺在这{y=-1的网格面}的(值)，我们得到一个神经元输出，一个非常接近-1的激活
| 而 对于x所有这{y=1的网格面}的值 我们得到{激活}值1
| 所以 在这个特定案例下 我们有一个人工神经元，(神经元)检测是否某个给定输入点"(x1,x2)"属于空间的这部分{y=-1的网格面}或者空间的那部分{y=1的网格面} 所以你可以考虑它{检测点为这或那部分}为 :
| 这个人工神经元作为一个二分类器，(该二分类器)分离在一个区域的点和其他区域的点
| 就现在 我们不打算讨论 我们如何找到神经元的参数，(这些参数)决定了 它(该神经元)分离了什么区域
| 就现在 我们只是假定 有人已经给我们权重向量w的值和偏执向量b的值，(w和b)决定了这个函数的形状
| {后面} 我们会看到 我们怎么能有神经元 {这些神经元}将{被}训练以找到这些参数的好值
| 所以就现在 我们只是假定有人给了我们这些{参数}值

| {几何解释} 几何，它{几何}发现 向量w其实会垂直于在空间中的超平面{二维时,y=0网格中间那条线为超平面,下文又将超平面称为ridge}，(此超平面)分割这两个区域，此神经元分割(这两个区域)
| 所以 这个向量w实质上决定了此ridge的方向，在空间的这两部分之间的(此ridge)，此神经元分割的(空间的这两部分)
| 这个偏执b本质上决定了沿着这个方向(ridge的垂直方向), 此ridge在哪{此ridge移动多少} 
| {ridge的垂直方向即w的方向}
| 当偏执b增大, ridge会沿着w反方向移动
| 所以这{ridge的移动大小和方向}是根据-b的 
| 如果你有更大的正b值 则ridge会以w反方向移动
| 我不会精确解释这是为什么 但是你可以坐下来 试着指出 为什么w垂直于此ridge 为什么b增加会使此ridge沿着w反方向移动
| 这就是此计算的描述，此人工神经元执行的(计算)

