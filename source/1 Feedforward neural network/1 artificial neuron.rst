.. 注释 ():同位语  []:省略了 但其实是不可少的  {}:译者的话

人工神经元
==========================================================

.. toctree::

| 在这个视频，我们将介绍人工神经元的概念，它是基本建筑块，我们将用(基本建筑块)构建复杂神经网络

.. image:: picture/1_01_artificial_neuron.pdf.page2.png 
   :scale: 50%

| 一个人工神经元是简单的一个计算单元,它(计算单元)将做一个特殊计算,(特殊计算)基于其他单元,它(计算单元)连到[其他单元]
| 上图是单人工神经元案例
| 它(神经元)直连在一个某个对象的输入描述，我们想抽取信息从[某个对象]
| 这个输入描述，我们叫它x
| 这儿两个x本质上是一个向量, (向量)包含标量x1到xd, d是此向量大小
| 注意，我将用粗体来写向量和矩阵 并且 每当无(粗体)时对应到一个标量值
| 所以xi是向量x的第i个元素
| 这个神经元将读信息从x向量并执行一个特殊计算，它(特殊计算)将决定神经元的值 
| 并且这个值将完全表明，是否某个特殊特征或信息出现[与否]在此对象，关于此主题(的特征或信息)，我们正在操纵的(主题)，被向量x描述的(对象)
| 现在 将执行的计算 能被拆成两步
| 此神经元的预激活计算会是第一个
| 有时 在你看讲义时 你看到输入激活的表达(相对预激活) 
| 但是 我将避免用输入激活 因为对于x我们经常也用词输入去描述
| 所以 我讨论这为预激活
| 并且 预激活，我记(预激活)作a(x)，对于一个给定的神经元
| ( a(x) )简单地会是标量b + 权重向量w乘输入向量x, b是神经元偏执，
| 所以 我们可以写它(式子b+ sigma w*x)成向量形式
| 或者 我们能写它(式子)成多标量形式,我们有 b + 对于所有向量内下标i w的第i个元素乘以x的第i个元素
| 然后 从这个(预激活a(x)=b+w*x) 神经元将计算它的激活，(激活)有时是作为神经元输出激活 按照 简单取预激活传递给激活函数g 
| g函数就是激活函数 并且 我们将可能用不同的激活函数

| 输出激活我经常所知它是h(x) . 如果我不说输出那就只剩激活了 . 如果我只说激活, 我的意思是一个给定神经元的输出激活
| {完成h(x)=g(a(x))=g(b+wx)的解释:} 所以 如果我只放a(x)在g中 很明确地 激活函数g将应用在输入向量的线性变换上，(向量)由元素xi组成

| {解释权重w} w我们将其作为连接权重的向量 
| 这是因为 在这里的可视化{上图} 我们可以把w的元素当作神经元间连接的强度 ,其他神经元(图中有曲线的神经元)到本神经元(x1)的连接 
| 这个案例中 这些(图中xi)神经元是指输入神经元 这里(图中xi)是神经元,(神经元)取输入向量中每一个元素的值 
| 所以w是连接权重的向量 
| {解释偏执b} b是偏执 因为如果我们没有输入{看公式a(x)=b+sigma w*x} b将是预激活
| 观察一个特定输入{w非0} 我们将完全远离初始值b, 神经元预激活的(初始值b)  然后像我说过的 g是激活函数


.. image:: picture/1_01_artificial_neuron.pdf.page3.png 
   :scale: 50%
| 这是一个神经元激活的二维可视化 
| 想象我们有一个向量, (该向量)由两个元素x1和x2组成
| 我们注意y 这我应写h(x)  (而不是y) 作为神经元的输出激活
| 这{y轴}我们注意到的第一个事情 是 在这个轴这 我们获得值,介于-1到+1之间(的值) 
| 这是因为 在这个特定例子中 我们能获得的值{激活}的范围  一旦我们传递预激活通过激活函数{从而能获得的激活值} 是一个数 (数)在-1到+1之间

