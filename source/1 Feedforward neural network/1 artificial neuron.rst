.. 注释 ():同位语  []:省略了 但其实是不可少的  {}:译者的话  "(x1,x2)":平面上的点坐标为(x1,x2)

人工神经元
==========================================================

.. toctree::

| 在这个视频，我们将介绍人工神经元的概念，人工神经元是基本建筑块，我们将用基本建筑块构建复杂神经网络

.. image:: picture/1_01_artificial_neuron.pdf.page2.png 
   :scale: 50%

| 一个人工神经元是简单的一个计算单元,该计算单元连到其他单元并将做一个基于其他单元的特殊计算
| 上图是单人工神经元案例
| 我们想从其抽取信息的某个对象的一个输入描述直连到神经元，
| 这个输入描述，我们叫它x
| 这儿两个x本质上是一个向量, 该向量大小为d并包含标量x1到xd, 
| 注意，向量和矩阵我将用粗体来书写，而每当非粗体时表示一个标量值
| 所以xi是向量x的第i个元素
| 这个神经元将从x向量读信息并执行一个特殊计算，该特殊计算将决定神经元的值 
| 并且这个值将完全表示，在由向量x描述的对象中，关于我们正在操纵的主题的某个特定特征或信息是否出现，
| 而将执行的计算能被拆成两步
| 第一步会是此神经元的预激活计算
| 有时 在你看讲义时 看到表达"输入激活"而不是"预激活" 
| 但是 我将避免用输入激活 因为对于x我们经常也用词输入去描述
| 故我用预激活
| 对于一个给定的神经元，我记预激活为a(x)，
| a(x)是神经元偏执标量b + 权重向量w乘输入向量x, 
| 可以把式子b+ sigma w*x写成向量形式
| 或写成多标量形式,即 b + 对于所有向量内下标i w的第i个元素乘以x的第i个元素
| 算出预激活a(x)后，再算神经元的激活，取预激活a(x)传递给激活函数所得值作为神经元输出激活
| g函数就是激活函数，激活函数可能不同

| 据我所知输出激活是h(x) . 如果我省略"输出"那就只剩"激活"了 . 如果我只说"激活", 我的意思是一个给定神经元的"输出激活"
| {完成h(x)=g(a(x))=g(b+wx)的解释:} 综上 a(x)放在g中 显然是 激活函数g应用在由元素xi组成的输入向量的线性变换上，

| {解释权重w} 我们将w作为连接权重的向量 
| 这是因为 在这里的可视化{上图} 我们可以把w的元素当作神经元间连接的强度 ,其他神经元(图中有曲线的神经元)到本神经元(x1)的连接 
| 这个案例中 这些(图中xi)神经元是指输入神经元 这里(图中xi)是神经元,(神经元)取输入向量中每一个元素的值 
| 所以w是连接权重的向量 
| {解释偏执b} b是偏执 因为如果我们没有输入{看公式a(x)=b+sigma w*x} b将是预激活
| 观察一个特定输入{w非0} 我们将完全远离初始值b, 神经元预激活的(初始值b)  然后像我说过的 g是激活函数


.. image:: picture/1_01_artificial_neuron.pdf.page3.png 
   :scale: 50%
 
| 这是一个神经元激活的二维可视化 
| 想象我们有一个向量, (该向量)由两个元素x1和x2组成
| 我们注意y 这我应写h(x)  (而不是y) 作为神经元的输出激活
| 这{y轴}我们注意到的第一个事情 是 在这个轴这 我们获得值,介于-1到+1之间(的值) 
| 这是因为 在这个特定例子中 我们能获得的值{激活}的范围  一旦我们传递预激活通过激活函数{从而能获得的激活值} 是一个数 (数)在-1到+1之间
| 现在我们看到 对于x的不同值 我们得到一个不同输出 
| 并且 特别地 对于所有值，躺在这{y=-1的网格面}的(值)，我们得到一个神经元输出，一个非常接近-1的激活
| 而 对于x所有这{y=1的网格面}的值 我们得到{激活}值1
| 所以 在这个特定案例下 我们有一个人工神经元，(神经元)检测是否某个给定输入点"(x1,x2)"属于空间的这部分{y=-1的网格面}或者空间的那部分{y=1的网格面} 所以你可以考虑它{检测点为这或那部分}为 :
| 这个人工神经元作为一个二分类器，(该二分类器)分离在一个区域的点和其他区域的点
| 就现在 我们不打算讨论 我们如何找到神经元的参数，(这些参数)决定了 它(该神经元)分离了什么区域
| 就现在 我们只是假定 有人已经给我们权重向量w的值和偏执向量b的值，(w和b)决定了这个函数的形状
| {后面} 我们会看到 我们怎么能有神经元 {这些神经元}将{被}训练以找到这些参数的好值
| 所以就现在 我们只是假定有人给了我们这些{参数}值

| {几何解释} 几何，它{几何}发现 向量w其实会垂直于在空间中的超平面{二维时,y=0网格中间那条线为超平面,下文又将超平面称为ridge}，(此超平面)分割这两个区域，此神经元分割(这两个区域)
| 所以 这个向量w实质上决定了此ridge的方向，在空间的这两部分之间的(此ridge)，此神经元分割的(空间的这两部分)
| 这个偏执b本质上决定了沿着这个方向(ridge的垂直方向), 此ridge在哪{此ridge移动多少} 
| {ridge的垂直方向即w的方向}
| 当偏执b增大, ridge会沿着w反方向移动
| 所以这{ridge的移动大小和方向}是根据-b的 
| 如果你有更大的正b值 则ridge会以w反方向移动
| 我不会精确解释这是为什么 但是你可以坐下来 试着指出 为什么w垂直于此ridge 为什么b增加会使此ridge沿着w反方向移动
| 这就是此计算的描述，此人工神经元执行的(计算)

