.. 注释     
.. {;xx} : xx是译者添加的 作为补充   
.. ~.xx.~  {}:针对xx 译者要说的话          
.. ~.xx.~  {.}:针对xx 演讲者当时的动作         ;  "xx"表示演讲者标出了讲义中的xx 
.. ~.xx.~  #.yy.# : 译者觉得要将xx换成yy才是对的
.. ~.xx.~  #..yy.# : 译者觉得 youtube自动翻译造成的错误          
.. #-.yy.# : 译者觉得 作者说错话了 试图用后面的词 来替代 yy    
.. ??yy??  : 译者没听出来 并用yy来占位
.. ~.xx.~  ~..yy..~:  xx和yy意思是一样的。本来只需要xx出现，或yy出现，不需要同时出现。比如为了不同角度的表达


多层神经网络
==========================================================

.. toctree::








.. image:: picture/1_04_multilayer_neural_network.pdf.page2-xx.png
   :scale: 50%

| 在这个视频中 我们将正式介绍多层神经网络.
| 我们知道 之前 有特定问题 单人工神经元不能很好建模,
| 因为 它是一个 线性不可分的 分类 问题.
| 但是 我们已经明白  我们可以 取 输入表示 :	 试图 以某方式 靠应用某个简单转化 {.右图像 x y ; AND AND}  转化输入.
| {move up}
| 如同应用 这个例子中 这的"与"函数{.右图像 y ; AND}:	   
|   {该"与"函数}转化输入
|   {靠"与"函数}获得输入的 新表示     线性可分的 新表示    
| 如果这个表示{.右图像 x y ; AND AND } 的计算 能被 人工神经元 做
| 那么 对于更复杂的问题 {.左图像 }   建议用 一个更复杂的模型 {.左图像 } ,
|     该模型:    先计算    一些  计算新表示的 人工神经元 {.右图像 x y; AND AND}  
| 再连接这些神经元到一个输出神经元 {.右图像 XOR }  该输出神经元会做 剩下的工作, 
| 从而 完成 更复杂函数的计算.
| 这就是 多层神经网络的使用和开发 背后的 灵感




.. image:: picture/1_04_multilayer_neural_network.pdf.page3-shlnn.png
   :scale: 50%
   
| 让我们 在单隐藏层的情况下 正式地看下这个想法 .
| 隐藏层的意思是 让问题更线性可分的表示 的计算.
| 在单层神经网络中 我们有第一部分{右图 底两层神经元} 该部分计算表示.
| 最后是 我们的输出单元{右图 顶层神经元} 该部分将执行二分类计算.
| 更正式地 当我们计算隐藏层时 我们会计算一个预激活函数{左公式 "pre-activation"}.
| 现在 在这个隐藏层{右图 中层神经元}中 我们有若干个神经元而不是一个神经元.
| 所以 每个第i个神经元{右图 中层神经元 h(x)i:"i"}, 我会叫它的 激活函数 输出激活 为 h(x)i, 该神经元连接到所有可能输入{右图 底层神经元}
| {move up}
| 所以 在第i个隐藏神经元{右图 底两层之间 W(1)i,j:"i"} 和 第j个输入{右图 底层第j个} 之间 的连接 包含在某个矩阵中, 该矩阵叫W上标(1) {右图 底两层之间 W(1)i,j}.
| {move up}
| 所以 这{右图 底两层之间 W(1)i,j:"(1)"}意味着 我们仅有一层隐藏层 的情况.
| 所以 W1是第一隐藏层的权重矩阵.
| 类似地 我们有偏执 {右图 底两层之间 b(1)i}.
| 所以 第i个隐藏单元的偏执是   bi   (1)   {右图 底两层之间 b(1)i:"(1)"}   , 上标1再次表示第一个隐藏层.
| {开始解释左侧}
| 每当我们计算预激活, 我们实际上得到一个预激活值向量{左 "a(x)=b(1)+W(1)x":"a(x)"}, 
|   该预激活值向量的第i个元素{"a(x)i=b(1)i+sum W(1)i,j xj":"a(x)i":"i"}是隐藏层中第i个神经元的预激活,
|   第i个预激活值为 第i个偏执 加 所有输入xj的权重组合,
|       该组合 为 xj被乘以 它和第i个神经元的连接{;的权重}.
| 如果我们想 对每个隐藏单元 执行这个计算{"a(x)i=b(1)i+sum W(1)i,j xj"}.
| 事实上 我们要把它{"a(x)i=b(1)i+sum W(1)i,j xj"}写成矩阵形式{"a(x)=b(1)+W(1)x"} 以 直接获得 预激活值向量{"a(x)=b(1)+W(1)x":"a(x)"}.
| 这{"a(x)=b(1)+W(1)x"}仅仅完全根据取向量x{"a(x)=b(1)+W(1)x":x}乘它以矩阵w(1) {"a(x)=b(1)+W(1)x":w(1)} 再 加上 整个 偏执向量b(1)  {"a(x)=b(1)+W(1)x":b(1)}.
| 确实  如果我们取x{矩阵}乘以w{矩阵},  我们得到的是 对于第i行 将得到一项,该项等于矩阵w1的那行的所有元素乘以向量x对应元素的和 
| {move up} 
| {move up} 
| 我们明白这{"a(x)=b(1)+W(1)x"}和这{"a(x)i=b(1)i+sum W(1)i,j xj"}实际上是等价的.
| {开始讲 左侧 公式 h(x)=g(a(x))}
| 然后 去 计算 隐藏层激活值 {"Hidden layer activation"}.
| 我们简单地 对{;预激活值}向量{"h(x)=g(a(x))":a(x)}中的所有元素 应用激活函数{"h(x)=g(a(x))":g}.
| 这 在我的符号{"h(x)=g(a(x))":g(a(x))}中, 我假定我们对每个元素应用激活函数g{"h(x)=g(a(x))":g} , 激活函数我们选择sigmoid或tanh或reclin等.










